{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python396jvsc74a57bd0ca795b0b969af7552bbfb8e0c2e566f97992b3bb2812b041a41eabd29862b494",
   "display_name": "Python 3.9.6 64-bit ('py39t': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "ca795b0b969af7552bbfb8e0c2e566f97992b3bb2812b041a41eabd29862b494"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/helena/miniforge3/envs/py39t/lib/python3.9/site-packages/nilearn/datasets/__init__.py:86: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n  warn(\"Fetchers from the nilearn.datasets module will be \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nilearn import plotting, datasets\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of the dataset\n",
    "\n",
    "# The data shared for NMA projects is a subset of the full HCP dataset\n",
    "N_SUBJECTS = 100\n",
    "\n",
    "# The data have already been aggregated into ROIs from the Glasser parcellation\n",
    "N_PARCELS = 360\n",
    "\n",
    "# The acquisition parameters for all tasks were identical\n",
    "TR = 0.72  # Time resolution, in seconds\n",
    "\n",
    "# The parcels are matched across hemispheres with the same order\n",
    "HEMIS = [\"Right\", \"Left\"]\n",
    "\n",
    "# Each experiment was repeated twice in each subject\n",
    "RUNS   = ['LR','RL']\n",
    "N_RUNS = 2\n",
    "\n",
    "# There are 7 tasks. Each has a number of 'conditions'\n",
    "# TIP: look inside the data folders for more fine-graned conditions\n",
    "\n",
    "EXPERIMENTS = {\n",
    "    'MOTOR'      : {'cond':['lf','rf','lh','rh','t','cue']},\n",
    "    'WM'         : {'cond':['0bk_body','0bk_faces','0bk_places','0bk_tools','2bk_body','2bk_faces','2bk_places','2bk_tools']},\n",
    "    'EMOTION'    : {'cond':['fear','neut']},\n",
    "    'GAMBLING'   : {'cond':['loss','win']},\n",
    "    'LANGUAGE'   : {'cond':['math','story']},\n",
    "    'RELATIONAL' : {'cond':['match','relation']},\n",
    "    'SOCIAL'     : {'cond':['mental','rnd']}\n",
    "}\n",
    "\n",
    "subjects = np.loadtxt('hcp_task/subjects_list.txt',dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = np.load(f\"hcp_task/regions.npy\").T\n",
    "region_info = dict(\n",
    "    name=regions[0].tolist(),\n",
    "    network=regions[1],\n",
    "    hemi=['Right']*int(N_PARCELS/2) + ['Left']*int(N_PARCELS/2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_timeseries(subject, experiment, run, remove_mean=True):\n",
    "  \"\"\"Load timeseries data for a single subject and single run.\n",
    "  \n",
    "  Args:\n",
    "    subject (str):      subject ID to load\n",
    "    experiment (str):   Name of experiment \n",
    "    run (int):          (0 or 1)\n",
    "    remove_mean (bool): If True, subtract the parcel-wise mean (typically the mean BOLD signal is not of interest)\n",
    "\n",
    "  Returns\n",
    "    ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
    "\n",
    "  \"\"\"\n",
    "  bold_run  = RUNS[run]\n",
    "  bold_path = f\"hcp_task/subjects/{subject}/{experiment}/tfMRI_{experiment}_{bold_run}\"\n",
    "  bold_file = \"data.npy\"\n",
    "  ts = np.load(f\"{bold_path}/{bold_file}\")\n",
    "  if remove_mean:\n",
    "    ts -= ts.mean(axis=1, keepdims=True)\n",
    "  return ts\n",
    "\n",
    "\n",
    "def load_evs(subject, experiment, run):\n",
    "  \"\"\"Load EVs (explanatory variables) data for one task experiment.\n",
    "\n",
    "  Args:\n",
    "    subject (str): subject ID to load\n",
    "    experiment (str) : Name of experiment\n",
    "    run (int): 0 or 1\n",
    "\n",
    "  Returns\n",
    "    evs (list of lists): A list of frames associated with each condition\n",
    "\n",
    "  \"\"\"\n",
    "  frames_list = []\n",
    "  task_key = f'tfMRI_{experiment}_{RUNS[run]}'\n",
    "  for cond in EXPERIMENTS[experiment]['cond']:    \n",
    "    ev_file  = f\"hcp_task/subjects/{subject}/{experiment}/{task_key}/EVs/{cond}.txt\"\n",
    "    ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)\n",
    "    ev       = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n",
    "    # Determine when trial starts, rounded down\n",
    "    start = np.floor(ev[\"onset\"] / TR).astype(int)\n",
    "    # Use trial duration to determine how many frames to include for trial\n",
    "    duration = np.ceil(ev[\"duration\"] / TR).astype(int)\n",
    "    # Take the range of frames that correspond to this specific trial\n",
    "    frames = [s + np.arange(0, d) for s, d in zip(start, duration)]\n",
    "    frames_list.append(frames)\n",
    "\n",
    "  return frames_list\n",
    "\n",
    "def load_stats(subject, experiment, run):\n",
    "    task_key = 'tfMRI_'+experiment+'_'+['RL','LR'][run]\n",
    "    for cond in EXPERIMENTS[experiment]['cond']:\n",
    "        file  = f\"hcp_task/subjects/{subject}/{experiment}/{task_key}/EVs/Stats.txt\"\n",
    "        stats = pd.read_table(file,header=None,delim_whitespace=True)\n",
    "        RT=np.array([stats.loc[0,3],stats.loc[1,3]])\n",
    "    return RT\n",
    "\n",
    "def average_frames(data, evs, experiment, cond):    \n",
    "    idx = EXPERIMENTS[experiment]['cond'].index(cond)\n",
    "    return np.mean(np.concatenate([np.mean(data[:,evs[idx][i]],axis=1,keepdims=True) for i in range(len(evs[idx]))],axis=-1),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_exp  = 'SOCIAL'\n",
    "\n",
    "group_contrast = []\n",
    "RT_group=[]\n",
    "\n",
    "for s in subjects:\n",
    "  for r in [0,1]:\n",
    "    data = load_single_timeseries(subject=s,experiment=my_exp,run=r,remove_mean=True)\n",
    "    evs = load_evs(subject=s, experiment=my_exp,run=r)\n",
    "    rt = load_stats(subject=s, experiment=my_exp,run=r)\n",
    "\n",
    "    m_activity = average_frames(data, evs, my_exp, 'mental')\n",
    "    r_activity = average_frames(data, evs, my_exp, 'rnd')\n",
    "\n",
    "    contrast    = m_activity-r_activity\n",
    "    group_contrast.append(contrast)\n",
    "    \n",
    "    RT_group.append(rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_contrast=np.array(group_contrast)\n",
    "RT_group=np.array(RT_group)\n",
    "RT_d=(RT_group[:,0]-RT_group[:,1])/((RT_group[:,0]+RT_group[:,1])/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(group_contrast,RT_d)\n",
    "RT_pred=reg.predict(group_contrast)\n",
    "\n",
    "print(r2_score(RT_d,RT_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-0.9852546848412795\n2.2186257019888895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(group_contrast, RT_d, test_size=0.5, random_state=100)\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X_train,y_train)\n",
    "y_pred=reg.predict(X_test)\n",
    "\n",
    "print(r2_score(y_test,y_pred))\n",
    "print(mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}